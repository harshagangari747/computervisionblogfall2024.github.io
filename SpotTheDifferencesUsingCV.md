<h2 align="center"> <b></b>Solving 'SPOTTING THE DIFFERENCES' puzzle using Computer Vision </h2></p>
<p align="center"><i><b>Sai Harsha Gangari, &nbsp; Sairam Reddy Malreddy</b></i></p>

<h4>Introduction</h4>
<p>Spotting the differences in two similar images is a fun game to solve. But sometimes, this game can become more difficult and tricky to solve as an adult. We got an idea to solve this game using computer vision techniques and advancements that are made in the recent years. This game is all about spotting the objects which are modified in some or the other way. For example: change in color, missing object or misplaced object from one image etc. This lead us to chose a object detection model called <b>YOLO</b> which was released in the year 2020.</p>
<p> We designed a website that gives a user a pair of 'similar' images. The user needs to detect the objects that are different in both the images. If the user can't figure out any differences, we give a set of 'clues' to guide the user to spot the differences. The image and clue generation is made using computer vision models; YOLO and DALL-E</p>

<h4>Objective</h4>
<p>Given a pair of two similar images, the objective is to generate clues to guide player to spot the differences in both the images. The clues could be about the objects itself or position of the objects</p>

<h4>Approach</h4>
<p>Our approach includes three steps.
<ul>
  <li>Get the pair of two 'similar' images from DALL-E by prompting the description of the image scene</li>
  <li>Using <i><b>'opencv'</b></i> python package, find the pixel differences in both the images</li>
  <li>Get the contour regions as the 'region of interest' in the difference-image</li>
  <li>Find the object in both the images at the given contour regions using YOLO model</li>
  <li>From the obtained objects that were detected, generate clues</li>
  </ul>
</p>

<h4>Architecture</h4>
<p> The following image briefly depicts the architecture of the program</p>
<div align="center">
<img width="50%" align="center" src="https://github.com/user-attachments/assets/c7ba3502-03e8-464b-9bd5-db4cdcc948e7" alt="Architecture Diagram" />
<br/>
<p><b>Figure 1: Architecture Diagram</b></p></div>

<h4>Issues</h4>
<p>
  <ul>
    <li>We realized that the YOLO model can only detect certain objects and the list of those objects is not so long. So, we decided to include only those objects that could be detected by YOLO in the DALL-E generated images.</li>
    <li>The images generated by DALL-E are insanely similar, but every time we tried to generate an image, one of the image is shifted either right or towards the left, which we believe that DALL-E is assuming the 'shifted objects' as differences.</li>
  </ul>
  
</p>

<br/>
<p><i>Enough of the technical details. Let's shall dive into demonstration</i></p>
<br/>
<h5>Prompting DALL-E to get two similar images</h5>
<p>We need to give a reference image to DALL-E to make it understand that we are seeking two similar pictures with minor differences in it. Else DALL-E struggles to understand the context and gives images which are essentially different. We prompted DALL-E with the below detailed description of the objects to include init along with a reference image</p>
<p>Here is a detailed textual description of the image prompted to DALL-E</p>
<pre>Create an image of a scenery/landscape by taking the reference of above image. 
Use only objects from this list 0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'.

Two image should be similar and copy of each other. But make changes to few objects for example, change the position of an object, change color of an object, delete an object, add a new object from the list. When I overlap the images, there should not be any pixel shift of the images. There should exactly align most of the static objects when overlapped. </pre>

<p>Here is the reference image that we gave along with the prompt to DALL-E </p>
<div align='center'><img width='50%' src = 'https://github.com/user-attachments/assets/7c97e6ff-4879-458f-b1a4-6cc494593156'/></div>

<p>And here is the image generated by the DALL-E</p>
<div align='center'><img width= '50%' src = 'https://github.com/user-attachments/assets/c75c91d9-cc21-42ee-89e7-9df948413b7a'/></div>

<p>When we used this image and fed into the model we got the output clues as</p>






